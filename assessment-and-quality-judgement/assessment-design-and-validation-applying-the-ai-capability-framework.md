# Assessment Design and Validation: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports **assessment design and validation processes** in education and professional learning contexts where assessment decisions must be **educationally sound, fair, defensible, and governance-credible**.

Assessment is one of the highest-stakes sites of AI use. Decisions made here directly affect learner progression, credibility of qualifications, institutional reputation, and regulatory compliance. When AI is introduced into assessment design — whether to generate tasks, suggest criteria, map learning outcomes, or validate alignment — the risks are not primarily technical. They are **pedagogical, ethical, and judgement-based**.

The purpose of this scenario is to help educators and institutions use AI, if at all, as a **bounded design support**, while ensuring that:
- academic standards remain human-defined
- assessment validity is not inferred or automated
- responsibility for judgement is explicit and documented

This scenario is designed to support:
- Programme and module leaders
- Assessment designers and learning designers
- Validation and approval panels
- Quality assurance and academic governance roles

---

## 2. Situation & Context

An assessment (or set of assessments) is being designed, revised, or reviewed. This may occur during:
- new programme or module development
- curriculum revalidation or periodic review
- assessment redesign in response to AI availability
- concerns about academic integrity or inclusivity

Typical pressures at this stage include:
- time constraints and approval deadlines
- pressure to “AI-proof” assessments quickly
- uncertainty about acceptable AI use by students
- tension between innovation and defensibility

AI may be proposed to:
- generate draft assessment tasks or prompts
- suggest alternative assessment formats
- map learning outcomes to criteria
- check alignment with standards or benchmarks

How AI is used — or resisted — at this stage will shape not only the assessment itself, but how confidently it can later be defended to students, external examiners, regulators, or legal challenge.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI is commonly introduced into assessment design to:
- draft assessment briefs or task descriptions
- generate marking criteria or rubrics
- suggest authentic or “AI-resistant” task formats
- summarise programme learning outcomes for alignment checks

These uses matter because:
- AI-generated tasks may privilege generic cognitive skills over disciplinary judgement
- rubric language may encode hidden assumptions or bias
- alignment checks can create a false sense of validity
- “AI-proofing” can shift focus from learning to control

This scenario treats AI use in assessment design as **high-risk and high-impact**, requiring deliberate slowing-down rather than acceleration.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI in assessment design, teams should clarify:

- the **educational purpose** of the assessment
- what evidence of learning genuinely matters
- where judgement, interpretation, or creativity are essential
- which risks would be unacceptable if challenged externally

Key awareness questions:
- What is this assessment actually trying to evidence?
- What must *not* be delegated or inferred?
- Where could AI introduce false confidence or compliance theatre?
- What assumptions about learners, disciplines, or norms might be embedded?

AI should not be used to resolve uncertainty about assessment purpose — that uncertainty must be surfaced and addressed explicitly.

---

### 4.2 Human–AI Co-Agency

In assessment design:
- humans remain the **authors and owners** of assessment intent
- AI may assist with drafting or comparison, not with validation

Good co-agency means:
- assessment parameters are defined *before* AI is used
- AI outputs are treated as prompts or alternatives
- design decisions are debated and recorded by humans

Avoid:
- accepting AI-generated tasks as “good enough”
- allowing AI to determine what counts as valid evidence
- framing AI suggestions as neutral or objective

Assessment legitimacy depends on **traceable human judgement**.

---

### 4.3 Applied Practice

Appropriate AI uses include:
- generating multiple draft task framings for comparison
- stress-testing whether tasks could be misinterpreted or shortcut
- identifying potential accessibility or workload concerns
- prompting questions about alignment rather than asserting it

Inappropriate uses include:
- generating final assessment tasks without redesign
- auto-producing rubrics treated as authoritative
- validating assessment quality through AI synthesis
- designing assessments primarily to defeat AI use

AI should support **reflective design**, not defensive optimisation.

---

### 4.4 Ethics, Equity & Impact

Assessment design shapes who succeeds, who struggles, and why.

Use the Framework to ask:
- Who might be advantaged or disadvantaged by this task?
- Does AI-assisted design privilege certain cultural or linguistic norms?
- Are we shifting burden onto students without support?
- Could this assessment increase anxiety, surveillance, or exclusion?

Ethical assessment design prioritises **fair opportunity to demonstrate learning**, not performative robustness.

---

### 4.5 Decision-Making & Governance

Strong governance of assessment design includes:
- clear ownership of assessment decisions
- documented rationale for task formats and criteria
- alignment with institutional and regulatory requirements
- defensibility to external scrutiny

If AI is used:
- record *where* and *how* it informed design
- ensure final decisions are human-endorsed
- avoid opaque or unreviewable design artefacts

Assessment validation is not a technical check — it is a governance responsibility.

---

### 4.6 Reflection, Learning & Renewal

After assessment design or validation, reflect:
- Did AI use clarify or obscure educational intent?
- Where did design decisions rely most on judgement?
- What risks remain, and are they acceptable?
- What guidance should be updated for future designs?

Reflection ensures assessment practice evolves with capability, not panic.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- What learning evidence would truly convince us this assessment works?
- Where are we compensating for uncertainty with complexity?
- What would we struggle to defend if challenged?

**Optional AI prompts**
- “Generate alternative task designs that emphasise different kinds of learning.”
- “Identify assumptions embedded in this assessment brief.”

**Pause & check**
- Are we designing for learning, or for control?
- Is AI helping us think better — or just faster?

---

## 6. After-Action Reflection

Once assessments are approved or implemented:
- Did students interpret tasks as intended?
- Where did unexpected behaviours or misunderstandings arise?
- Did AI use create confidence or complacency?
- What should be revised before the next cycle?

Feed learning into assessment review and governance processes.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- design assessments that remain valid in AI-rich environments
- avoid false assurance from AI-assisted validation
- strengthen academic and governance confidence
- surface ethical and equity implications early
- build durable, defensible assessment capability
