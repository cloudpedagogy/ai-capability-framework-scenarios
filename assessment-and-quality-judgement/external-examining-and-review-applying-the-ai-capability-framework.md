# External Examining and Review: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports **external examining and independent review processes** where academic standards, assessment practices, and learner outcomes are scrutinised by individuals who are intentionally positioned outside the delivery team.

External examining plays a critical role in:
- safeguarding academic standards
- ensuring comparability across programmes and institutions
- providing independent assurance to regulators and the public

As AI becomes more embedded in assessment design, marking, feedback, and moderation, external examiners are increasingly confronted with **AI-shaped practices** that may be unevenly documented, poorly articulated, or defensively framed.

The purpose of this scenario is to help institutions and external examiners engage with AI-influenced assessment **openly, critically, and constructively**, while preserving independence, rigour, and trust.

This scenario is designed to support:
- External examiners and reviewers
- Programme and assessment leads
- Quality assurance and academic governance staff
- Committees responsible for standards and awards

---

## 2. Situation & Context

An external examining or review process is underway. This may involve:
- sampling student work and feedback
- reviewing marking and moderation practices
- attending assessment boards or panels
- producing an independent report on standards

In AI-influenced contexts, additional complexity may include:
- AI-assisted marking or feedback
- AI-supported calibration or analysis
- AI-aware assessment design
- uncertainty about how AI use is documented or disclosed

Typical pressures include:
- time-limited review windows
- uneven clarity about AI practices
- anxiety about regulatory scrutiny
- fear of appearing either naïve or over-confident about AI

How AI use is framed during external examining strongly affects **credibility, defensibility, and trust**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may appear in external examining contexts in several ways:
- as part of the assessment lifecycle under review
- as a tool used by programme teams to prepare documentation
- as a sensemaking aid for examiners reviewing large datasets
- as a topic of concern or challenge raised in reports

These uses matter because:
- undocumented AI use undermines transparency
- over-polished narratives can obscure risk
- AI summaries may flatten nuance or dissent
- inconsistent explanations damage confidence

This scenario treats AI in external examining as **high governance sensitivity**, even when direct AI use is minimal.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before engaging with AI-influenced assessment during review, examiners and institutions should clarify:

- where AI is genuinely shaping assessment practice
- what has changed compared to previous cycles
- what is provisional, experimental, or established
- what risks are known versus assumed

Key awareness questions:
- What exactly is the examiner being asked to assure?
- Where does AI intersect with standards, not just process?
- What uncertainty should be surfaced rather than resolved?
- Are we explaining practice — or defending it?

AI should not be used to project confidence where it does not yet exist.

---

### 4.2 Human–AI Co-Agency

In external examining:
- humans remain fully responsible for judgement and assurance
- AI must never substitute for independent scrutiny

Good co-agency means:
- institutions articulate AI use clearly and honestly
- examiners treat AI-supported artefacts as contextual inputs
- judgement remains grounded in academic standards and evidence

Avoid:
- presenting AI outputs as objective validation
- asking examiners to rely on AI-generated summaries alone
- positioning AI as a guarantor of quality

Independence depends on **human critical distance**, not computational support.

---

### 4.3 Applied Practice

Appropriate AI uses include:
- helping programme teams collate large volumes of evidence
- supporting examiners in navigating documentation
- flagging areas for deeper human review

Inappropriate uses include:
- summarising away disagreement or concern
- reframing examiner feedback through AI
- pre-emptively “normalising” risk patterns

AI should support **visibility**, not narrative control.

---

### 4.4 Ethics, Equity & Impact

External examining has sector-wide implications.

Use the Framework to ask:
- Does AI use affect comparability across cohorts or institutions?
- Are certain learner groups disproportionately impacted?
- Are experimental practices being piloted responsibly?
- Is transparency being prioritised over reassurance?

Ethical review requires honesty about limits, not optimisation of appearance.

---

### 4.5 Decision-Making & Governance

Robust governance in external examining includes:
- clear documentation of assessment processes
- explicit description of any AI-supported practices
- traceable responses to examiner feedback
- separation of assurance from advocacy

If AI is involved:
- document where it informs preparation or review
- ensure examiner reports remain unfiltered
- avoid retroactive justification through AI language

External reports must remain **independent records**, not co-authored artefacts.

---

### 4.6 Reflection, Learning & Renewal

After external examining cycles, reflect:
- Where did AI use prompt legitimate concern?
- What questions did examiners struggle to answer?
- What guidance or transparency is missing?
- How can future cycles be clearer and calmer?

Reflection supports institutional maturity, not reputational defence.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- Would an external examiner recognise this description as accurate?
- Where are we unsure — and have we said so?
- What evidence actually supports this claim?

**Optional AI prompts**
- “Summarise this assessment process, highlighting uncertainty and risk.”
- “List areas an external examiner might reasonably question.”

**Pause & check**
- Are we explaining practice or managing perception?
- Would less AI involvement increase trust here?

---

## 6. After-Action Reflection

Following examiner reports:
- Were AI-related concerns addressed substantively?
- Did AI use complicate or clarify assurance?
- Were responses proportionate and transparent?
- What should be standardised or retired?

Use learning to strengthen future cycles and documentation.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- engage external scrutiny with confidence and humility
- articulate AI-influenced assessment credibly
- avoid defensive or opaque narratives
- strengthen trust with examiners and regulators
- build durable, review-ready assessment practices
