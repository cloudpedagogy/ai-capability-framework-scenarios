# Marking, Moderation, and Calibration: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports **marking, moderation, and calibration processes** where judgements about learner performance are made, reviewed, and standardised across assessors, cohorts, or programmes.

These processes are among the most **trust-sensitive and risk-exposed** points in education and professional assessment. They directly affect grades, progression, awards, appeals, and institutional credibility. When AI is introduced — whether to assist with marking, summarise feedback, flag anomalies, or support calibration — the risks are not only technical. They concern **fairness, consistency, transparency, and accountability**.

The purpose of this scenario is to help institutions use AI, if at all, as a **bounded support for human judgement**, while ensuring that:
- academic standards remain assessor-led
- consistency is achieved through calibration, not automation
- accountability for marks is explicit and defensible

This scenario is designed to support:
- Academic markers and assessors
- Moderation and calibration panels
- Programme and assessment leads
- Quality assurance and academic governance roles

---

## 2. Situation & Context

Marking and moderation are taking place following an assessment cycle. This may involve:
- individual or team-based marking
- internal moderation or second marking
- calibration across markers or cohorts
- preparation for external examining or audit

Typical pressures at this stage include:
- large volumes of student work
- tight turnaround times
- variation in marker interpretation
- concern about AI use in student submissions
- fear of inconsistency or challenge

AI may be proposed to:
- assist with first-pass marking or classification
- summarise or standardise feedback language
- identify outliers or anomalies
- compare marking patterns across assessors

How AI is used — or resisted — at this stage will strongly influence **perceptions of fairness, legitimacy, and trust**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI is commonly introduced into marking and moderation to:
- generate indicative grades or classifications
- summarise qualitative feedback
- flag inconsistencies or unusual patterns
- support calibration discussions

These uses matter because:
- AI grading can obscure criteria interpretation
- feedback generation may flatten nuance or tone
- anomaly detection can stigmatise individuals
- calibration can shift from dialogue to compliance

This scenario treats AI use in marking and moderation as **high-risk**, particularly where summative decisions and appeals are possible.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI in marking or moderation, teams should clarify:

- what constitutes **academic judgement** in this context
- which aspects of marking require interpretation, not pattern matching
- how disagreement between markers is expected and resolved
- where AI could introduce false neutrality or authority

Key awareness questions:
- What decisions are we making that affect learner outcomes?
- Where does professional judgement matter most?
- What risks would be unacceptable if challenged by a student or regulator?
- Are we seeking support, or reassurance?

AI should not be used to avoid the discomfort of disagreement.

---

### 4.2 Human–AI Co-Agency

In marking and moderation:
- humans remain fully responsible for grades and feedback
- AI may assist with organisation or comparison, not judgement

Good co-agency means:
- markers understand and agree the role of AI in advance
- AI outputs are treated as advisory or exploratory
- final marks are explicitly human-assigned and owned

Avoid:
- delegating grading decisions to AI systems
- allowing AI summaries to override assessor reasoning
- using AI to “smooth out” disagreement without discussion

Consistency comes from **shared standards**, not shared tools.

---

### 4.3 Applied Practice

Appropriate AI uses include:
- comparing feedback tone or length across markers
- identifying potential inconsistencies for review
- supporting calibration conversations with examples
- helping markers reflect on their own patterns

Inappropriate uses include:
- auto-marking or score assignment
- replacing second marking or moderation
- generating final feedback without review
- ranking assessors or students algorithmically

AI should support **calibration dialogue**, not replace it.

---

### 4.4 Ethics, Equity & Impact

Marking practices have direct equity consequences.

Use the Framework to ask:
- Could AI amplify existing bias or disadvantage?
- Are certain writing styles or formats privileged?
- Might AI flagging create undue scrutiny or stress?
- How are students informed about marking processes?

Ethical marking prioritises fairness, dignity, and transparency — not efficiency alone.

---

### 4.5 Decision-Making & Governance

Strong governance of marking and moderation includes:
- clear marking criteria and shared standards
- documented moderation and calibration processes
- traceable decision-making for grades
- alignment with appeals and complaints procedures

If AI is used:
- record where it informed review or discussion
- ensure human ratification of all outcomes
- avoid opaque or irreproducible marking processes

Marks must always be defensible as **human academic decisions**.

---

### 4.6 Reflection, Learning & Renewal

After marking and moderation cycles, reflect:
- Where did judgement vary most — and why?
- Did AI use support or distort shared understanding?
- Were any patterns surprising or concerning?
- What guidance or training should be updated?

Reflection strengthens assessment culture, not just compliance.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- Would we be comfortable explaining this mark face-to-face?
- Where do criteria require interpretation rather than application?
- What evidence justifies this decision?

**Optional AI prompts**
- “Compare these feedback comments for tone and clarity without changing meaning.”
- “Highlight differences in how criteria are being interpreted.”

**Pause & check**
- Are we using AI to support judgement — or to avoid it?
- Would removing AI improve trust in this decision?

---

## 6. After-Action Reflection

Once marks are finalised:
- Were moderation processes perceived as fair?
- Did AI use reduce or increase confidence?
- Were any risks introduced unintentionally?
- How should practice evolve next cycle?

Feed learning into calibration guidance and assessor development.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- preserve academic judgement in AI-rich contexts
- strengthen fairness and consistency without automation
- reduce risk in appeals and external scrutiny
- support assessors through calibration, not surveillance
- build robust, trusted marking and moderation practices
