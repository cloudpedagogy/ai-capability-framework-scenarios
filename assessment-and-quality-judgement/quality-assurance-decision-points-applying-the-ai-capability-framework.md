# Quality Assurance Decision Points: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports **quality assurance (QA) decision points** where institutions make formal judgements about standards, risk, approval, or continuation of academic provision.

These moments include decisions such as:
- whether a programme meets required standards
- whether assessment or progression practices are acceptable
- whether risks are tolerable, mitigated, or unacceptable
- whether corrective action is required

As AI becomes embedded across teaching, assessment, and administration, QA bodies are increasingly asked to judge **AI-influenced systems and practices**, often under regulatory or reputational pressure.

The purpose of this scenario is to help QA decision-makers use AI **as an input to judgement, not a substitute for it**, ensuring decisions remain defensible, proportionate, and aligned with institutional values.

This scenario is designed to support:
- Quality assurance committees and panels
- Programme approval and review boards
- Academic standards and regulations committees
- Senior academic leaders with QA responsibilities

---

## 2. Situation & Context

A QA decision point is reached because:
- a programme is under review or reapproval
- concerns have been raised through monitoring or audit
- external examiners or regulators have flagged issues
- AI-related practices require formal judgement

At this stage:
- evidence is partial, aggregated, and interpretive
- data may be incomplete or lagging
- consequences of decisions are significant
- pressure exists to appear rigorous and decisive

AI may be introduced to synthesise evidence, identify patterns, or highlight risk. How it is used will shape whether QA decisions strengthen confidence or create false certainty.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used at QA decision points to:
- summarise large volumes of evidence
- flag risk indicators or anomalies
- compare programmes or outcomes across cohorts
- generate dashboards or decision briefs

These uses matter because:
- aggregation can hide contextual nuance
- risk scoring may privilege what is measurable
- comparative analytics can obscure structural differences
- dashboards can imply objectivity where judgement is required

This scenario treats AI use in QA decision-making as **high-stakes and governance-critical**.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI-supported evidence, QA bodies should clarify:
- what decision is actually being made
- what standards or thresholds apply
- what uncertainty is acceptable
- what cannot be resolved at this stage

Key awareness questions:
- Are we judging quality, compliance, or risk?
- What assumptions underlie the data we are seeing?
- What evidence is missing or contested?
- Where does professional judgement remain essential?

AI should not be used to close questions prematurely.

---

### 4.2 Human–AI Co-Agency

In QA contexts:
- humans remain accountable for standards and decisions
- AI may assist with synthesis, not verdicts

Good co-agency means:
- committees understand how AI outputs were generated
- members challenge patterns and indicators
- decisions are articulated in human reasoning, not metrics

Avoid:
- deferring judgement to dashboards
- treating risk flags as conclusions
- equating volume of data with certainty

QA legitimacy depends on visible human judgement.

---

### 4.3 Applied Practice

Appropriate AI uses include:
- collating and structuring evidence for review
- highlighting areas for focused discussion
- supporting scenario exploration (e.g. “if this continues…”)

Inappropriate uses include:
- automated pass/fail decisions
- ranking programme quality without context
- generating recommendations without deliberation

AI should support **deliberation**, not replace it.

---

### 4.4 Ethics, Equity & Impact

QA decisions have wide-ranging impact.

Use the Framework to ask:
- Who is affected by this decision, and how?
- Do metrics disadvantage certain disciplines or cohorts?
- Could AI-mediated evidence reinforce systemic bias?
- Are consequences proportionate to risk?

Ethical QA recognises that neutrality is not the same as fairness.

---

### 4.5 Decision-Making & Governance

Strong QA governance includes:
- clear articulation of decision criteria
- separation of evidence, interpretation, and judgement
- documentation of rationale and dissent
- alignment with regulatory expectations

If AI is used:
- record its role and limitations
- ensure decisions are traceable to human reasoning
- avoid creating opaque decision pathways

QA decisions must be **auditable, explainable, and defensible**.

---

### 4.6 Reflection, Learning & Renewal

After QA decisions, institutions should reflect:
- Did AI use improve clarity or obscure judgement?
- Were decisions understood and trusted?
- What capability gaps were revealed?
- How should QA processes evolve?

Reflection ensures QA remains adaptive rather than reactive.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- What judgement are we being asked to make?
- Where might data be misleading?
- What would proportional confidence look like here?

**Optional AI prompts**
- “Summarise areas of agreement and uncertainty in the evidence.”
- “Identify assumptions underlying these risk indicators.”

**Pause & check**
- Are we mistaking indicators for insight?
- Would we defend this decision publicly?

---

## 6. After-Action Reflection

Following QA decisions:
- Were outcomes communicated clearly?
- Did AI use affect confidence in the process?
- Were impacts monitored appropriately?
- What learning should inform future QA cycles?

Use insights to strengthen standards and governance.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- make AI-informed QA decisions without automation bias
- protect legitimacy of academic standards
- balance evidence with professional judgement
- integrate ethics and equity into QA processes
- build mature, governance-ready AI capability
