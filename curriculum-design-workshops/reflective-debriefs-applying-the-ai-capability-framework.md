# Reflective Debriefs — Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports structured reflective debriefs following teaching, learning, or facilitation activities where AI was present, permitted, constrained, or intentionally excluded. It focuses on making learning about AI use — and non-use — explicit, shared, and developmental.

Reflective debriefs are often informal or omitted altogether. When they do occur, they may focus narrowly on task outcomes rather than on how learning happened. This scenario reframes debriefs as a capability-building practice, helping learners and educators surface judgement, assumptions, and ethical considerations around AI use.

The purpose of this scenario is to help facilitators use reflection to consolidate AI capability, strengthen learning design, and inform future practice.

This scenario is designed to support:

- Educators and tutors  
- Learning designers and educational developers  
- Facilitators of workshops and training  
- Students participating in AI-aware learning activities  

---

## 2. Situation and Context

A learning activity, workshop, or session has concluded. Participants may have:

- used AI tools directly  
- chosen not to use AI  
- encountered ambiguity or tension around AI use  

At this point:

- experiences are still fresh  
- judgements and emotions are accessible  
- insights about design and facilitation are available  

AI may be used to support reflection (for example, clustering themes or prompting questions), but reflection itself must remain human-led and meaning-focused.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used in reflective debriefs to:

- summarise participant reflections  
- surface recurring themes or tensions  
- generate reflective prompts  

These uses matter because:

- summaries can smooth over disagreement  
- thematic analysis may privilege majority views  
- generated prompts can subtly steer reflection  

This scenario treats AI use in reflective debriefs as **low-risk but epistemically sensitive**, requiring careful framing.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before facilitating a debrief, clarify:

- the purpose of the reflection (learning, improvement, sensemaking)  
- what aspects of AI use are in scope  
- how insights will be used  

Key awareness questions:

- What are we trying to learn from this experience?  
- What surprised or challenged participants?  
- Where did judgement matter most?  

AI should be used to support articulation, not interpretation.

---

### 4.2 Human–AI Co-Agency

In reflective contexts:

- humans remain the interpreters of experience  
- AI may assist with organisation or prompting  

Good co-agency means:

- reflection questions are human-defined  
- AI outputs are treated as provisional summaries  
- participants can challenge or nuance synthesis  

Avoid:

- treating AI-generated themes as definitive  
- outsourcing meaning-making to tools  

---

### 4.3 Applied Practice

Appropriate AI uses include:

- clustering anonymised reflections for discussion  
- generating alternative reflective questions  
- supporting facilitators to notice patterns  

Inappropriate uses include:

- evaluating participant performance  
- scoring reflections or engagement  
- filtering out minority or uncomfortable views  

AI should support collective reflection, not judgement.

---

### 4.4 Ethics, Equity and Impact

Reflection must be psychologically safe.

Use the Framework to ask:

- Are all voices able to be heard?  
- Are power dynamics influencing reflection?  
- Could AI synthesis silence marginal perspectives?  

Ethical debriefs prioritise care, inclusion, and trust.

---

### 4.5 Decision-Making and Governance

Good governance of reflective practice includes:

- clarity about how reflections are recorded and used  
- appropriate anonymisation and data handling  
- alignment with institutional ethics and privacy policies  

If AI is used:

- explain its role transparently  
- avoid retaining unnecessary data  
- ensure reflections are not repurposed without consent  

This supports legitimacy and confidence in reflective processes.

---

### 4.6 Reflection, Learning and Renewal

This scenario directly activates the Framework’s renewal domain.

Key renewal questions:

- What capability has developed through this experience?  
- What should we do differently next time?  
- What patterns are emerging across activities?  

Reflection closes the loop between experience, learning, and improvement.

---

## 5. In-the-Moment Prompts and Checks

### Human reflection prompts

- Where did AI help or hinder learning?  
- What judgement did we have to exercise?  
- What questions remain unresolved?  

### Optional AI prompts

- “Summarise anonymised reflections, noting areas of disagreement.”  
- “Generate reflective questions that surface ethical or equity concerns.”  

### Pause and check

- Are we allowing space for discomfort and uncertainty?  
- Is reflection being rushed or instrumentalised?

---

## 6. After-Action Reflection

Following reflective debriefs, consider:

- What insights should inform future design?  
- What guidance or support needs updating?  
- How will learning be shared responsibly?  

Feed insights into curriculum, CPD, or facilitation practice as appropriate.

---

## 7. What This Scenario Delivers

This scenario helps organisations to:

- embed reflection as a core AI capability practice  
- surface tacit judgement and ethical reasoning  
- improve learning design through lived experience  
- avoid superficial evaluation of AI use  
- build sustainable, reflective AI capability cultures
