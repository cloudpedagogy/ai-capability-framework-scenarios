# Predictive Insight and Early Warning Systems: Applying the AI Capability Framework

## 1. Purpose of This Scenario

This scenario supports the use of AI-generated predictive insights and early warning systems in organisational contexts where leaders and teams seek to anticipate risk, opportunity, or emerging issues.

Predictive models, risk scores, alerts, and forecasts are increasingly used to inform decisions in areas such as student progression, workforce planning, financial risk, research performance, safety, and compliance. While these tools can surface patterns and prompt timely attention, they also carry significant risks: false certainty, hidden assumptions, automation bias, and premature intervention.

The purpose of this scenario is to help professionals use AI-generated predictive insights as **signals for judgement**, not substitutes for it — ensuring that interpretation, action, and accountability remain explicitly human.

This scenario is designed to support:
- Senior leaders and decision-makers
- Risk, planning, and assurance teams
- Data and analytics units
- Governance committees and boards
- Professionals responsible for acting on alerts or forecasts

---

## 2. Situation & Context

An organisation has access to predictive insights or early warning outputs, such as:
- risk scores or likelihood estimates
- automated alerts or flags
- trend extrapolations
- scenario forecasts
- anomaly detection systems

These insights may relate to:
- performance deterioration
- compliance or safety risk
- student or staff outcomes
- financial or operational pressure
- reputational exposure

Common pressures include:
- expectation of proactive action
- fear of missing early signals
- demand for decisive leadership
- limited time to interrogate model assumptions

AI may be presented as offering “advance notice” — but the meaning and reliability of that notice requires careful judgement.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used to:
- generate predictive risk scores
- trigger alerts or escalation
- prioritise cases or areas for attention
- simulate future scenarios
- recommend early interventions

These uses matter because:
- predictions may be probabilistic, not determinate
- alerts can reify assumptions as facts
- early intervention may itself create harm
- opaque models can obscure bias or error

This scenario treats AI use in predictive insight as **high-risk**, particularly where action affects people, resources, or reputations.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using predictive AI outputs, clarify:
- what the model is designed to predict (and what it is not)
- the data sources and their limitations
- known error rates, uncertainty, or bias
- what decisions the prediction is intended to inform

Key awareness questions:
- Is this a forecast, a signal, or a hypothesis?
- What assumptions underpin this model?
- Where might false positives or negatives occur?
- What contextual factors are invisible to the system?

AI should surface *possibilities*, not assert futures.

---

### 4.2 Human–AI Co-Agency

In predictive contexts:
- humans remain responsible for interpretation and action
- AI provides signals, not decisions

Good co-agency means:
- humans decide whether and how to respond
- AI outputs are contextualised and challenged
- responsibility for outcomes is explicit

Avoid:
- treating alerts as mandates
- assuming predictive accuracy equals authority
- deferring difficult judgement to automated scores

---

### 4.3 Applied Practice

Appropriate AI uses include:
- prioritising attention for human review
- highlighting areas for deeper investigation
- supporting scenario discussion and planning
- stress-testing assumptions under uncertainty

Inappropriate uses include:
- automatic escalation or intervention
- denying opportunities or support based on prediction alone
- treating forecasts as commitments
- bypassing professional judgement

AI should support *preparedness*, not pre-emption.

---

### 4.4 Ethics, Equity & Impact

Predictive systems can disproportionately affect people.

Use the Framework to ask:
- Who is most affected by being flagged or predicted?
- Could historical bias be embedded in the model?
- What harms might arise from early intervention?
- Are those affected aware and protected?

Ethical use requires caution, proportionality, and care.

---

### 4.5 Decision-Making & Governance

Strong governance practices include:
- clear thresholds for action versus monitoring
- documented rationale for responding to predictions
- regular review of model performance and impact
- oversight of automated alerting systems

If AI is used:
- record when and how predictions informed decisions
- ensure humans approve any consequential action
- avoid embedding predictive outputs into policy without review

This supports legitimacy and accountability.

---

### 4.6 Reflection, Learning & Renewal

After acting on predictive insights, reflect:
- Were predictions helpful or misleading?
- What unintended effects emerged?
- How did human judgement interact with AI signals?
- Should the system or its use be adjusted?

Reflection prevents predictive systems from becoming unquestioned authorities.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- What is this prediction actually telling us?
- What would happen if we did nothing?
- Who bears the risk of acting — or not acting?

**Optional AI prompts**
- “Explain the assumptions and uncertainty behind this prediction.”
- “Generate alternative futures that contradict this forecast.”

**Pause & check**
- Are we mistaking probability for inevitability?
- Would we make this decision without the model?

---

## 6. After-Action Reflection

Following responses to predictive insights:
- Did actions align with actual outcomes?
- Were people affected fairly and transparently?
- How should future predictions be handled differently?

Use learning to recalibrate both systems and judgement.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- use predictive AI responsibly and cautiously
- avoid automation bias and premature intervention
- protect fairness and human agency
- strengthen governance around early warning systems
- build mature AI capability for uncertain futures
