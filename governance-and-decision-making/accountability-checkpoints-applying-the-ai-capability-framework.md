# Accountability Checkpoints — Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports periodic accountability checkpoints where past decisions, practices, or AI-enabled processes are reviewed to ensure they remain appropriate, effective, and aligned with organisational values and responsibilities.

Accountability checkpoints differ from one-off reviews. They are deliberate moments of pause built into governance cycles to ask whether decisions made earlier — often with AI support — are still justified, well-governed, and producing intended outcomes.

AI may be introduced at this stage to collate evidence, track indicators, or summarise activity over time. While this can support oversight, it also risks normalising earlier decisions, obscuring drift, or creating automated reassurance if not carefully bounded.

The purpose of this scenario is to help professionals use AI to support reflective accountability, while ensuring that responsibility, judgement, and corrective action remain human-led.

This scenario is designed to support:

- Senior leaders and accountable officers  
- Governance, audit, and assurance committees  
- Programme and portfolio leads  
- Academic and professional services staff with oversight responsibilities  

---

## 2. Situation and Context

An accountability checkpoint is convened at a defined interval or trigger point. It may review:

- AI-enabled decisions or processes  
- implementation of approved policies or programmes  
- outcomes of earlier risk or ethics reviews  
- patterns of use, escalation, or exception  

These checkpoints often occur:

- after implementation, not design  
- when momentum has built around existing practice  
- under pressure to demonstrate assurance  

AI may be used to aggregate data, summarise trends, or flag issues. How it is used will shape whether accountability is genuine or performative.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used at accountability checkpoints to:

- summarise activity logs or records  
- identify trends, outliers, or changes over time  
- compare intended and actual practice  
- highlight potential compliance gaps  

These uses matter because:

- summaries can normalise problematic patterns  
- trend analysis may miss contextual explanation  
- automated flags can be treated as sufficient assurance  

This scenario treats AI use in accountability checkpoints as **medium- to high-risk**, particularly where reputational or legal responsibility is involved.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI, clarify:

- what is being held accountable, and to whom  
- what success, failure, or drift would look like  
- what evidence is meaningful versus merely available  

Key awareness questions:

- What decisions are we accountable for at this point?  
- What assumptions were made earlier that need re-examination?  
- Where might AI-generated summaries hide important context?  

AI should be used to surface questions, not to close them.

---

### 4.2 Human–AI Co-Agency

In accountability checkpoints:

- humans retain responsibility for judgement and action  
- AI may assist with evidence organisation and pattern detection  

Good co-agency means:

- accountability questions are human-defined  
- AI outputs are interrogated, not accepted  
- escalation and remediation decisions remain human  

Avoid:

- allowing AI to determine whether accountability has been met  
- delegating corrective judgement to automated signals  

---

### 4.3 Applied Practice

Appropriate AI uses include:

- consolidating evidence across time or systems  
- highlighting discrepancies between policy and practice  
- surfacing trends that warrant deeper review  

Inappropriate uses include:

- certifying compliance automatically  
- closing issues based solely on AI summaries  
- replacing qualitative review with quantitative proxies  

AI should support informed scrutiny, not automated assurance.

---

### 4.4 Ethics, Equity and Impact

Accountability checkpoints must consider impact.

Use the Framework to ask:

- Who has benefited or been disadvantaged by current practice?  
- Are harms or inequities emerging over time?  
- Does AI use obscure lived experience or marginal voices?  

Ethical accountability looks beyond metrics to real-world effects.

---

### 4.5 Decision-Making and Governance

Strong governance practices include:

- clear ownership of accountability outcomes  
- documentation of findings and actions  
- integration with risk, audit, and assurance cycles  

If AI is used:

- document its role and limitations  
- ensure findings are reviewed by accountable humans  
- avoid creating a false sense of closure  

This supports credible and defensible oversight.

---

### 4.6 Reflection, Learning and Renewal

Accountability checkpoints activate renewal.

Key renewal questions:

- What needs to change as a result of this review?  
- What capability gaps have become visible?  
- How should governance arrangements evolve?  

Learning at this stage prevents stagnation and drift.

---

## 5. In-the-Moment Prompts and Checks

### Human reflection prompts

- What are we still accountable for today?  
- Where has practice drifted from intent?  
- What would responsible correction look like?  

### Optional AI prompts

- “Summarise patterns of AI use over time, noting deviations from intended practice.”  
- “Highlight areas where outcomes differ from original assumptions.”  

### Pause and check

- Are we genuinely open to change?  
- Would this checkpoint stand up to external scrutiny?

---

## 6. After-Action Reflection

Following an accountability checkpoint, consider:

- What corrective actions are required?  
- Who is responsible for implementing them?  
- How will improvement be monitored?  

Ensure learning feeds back into policy, practice, and design.

---

## 7. What This Scenario Delivers

This scenario helps organisations to:

- maintain accountability over time  
- avoid governance drift and automated reassurance  
- surface long-term ethical and operational impacts  
- strengthen trust and legitimacy  
- build sustainable, reflective AI capability
