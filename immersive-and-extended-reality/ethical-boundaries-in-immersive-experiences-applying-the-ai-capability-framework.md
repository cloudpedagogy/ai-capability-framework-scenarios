# Ethical Boundaries in Immersive Experiences: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports decision-making around **ethical boundaries in immersive, AI-enabled experiences**, where participants are placed inside simulations that may evoke emotional, psychological, behavioural, or identity-level responses.

Immersive environments can be powerful tools for learning, training, and assessment — but they can also blur boundaries between simulation and reality, performance and identity, consent and compliance. When AI adapts scenarios, responds dynamically, or personalises experiences, these risks intensify.

The purpose of this scenario is to help organisations **set, recognise, and enforce ethical boundaries** in immersive experiences, ensuring participant safety, dignity, consent, and trust remain central.

This scenario is designed to support:

- Educators and training designers  
- Simulation and XR developers  
- Ethics committees and review boards  
- Safeguarding, wellbeing, and pastoral leads  
- Senior leaders responsible for innovation governance  

---

## 2. Situation & Context

An organisation is deploying immersive experiences for purposes such as:

- professional training or rehearsal  
- leadership or behavioural development  
- sensitive scenario exploration (e.g. crisis, conflict, trauma)  
- values, ethics, or decision-making simulations  

Within these experiences:

- participants may embody roles unlike themselves  
- scenarios may escalate dynamically via AI  
- emotional or stress responses may be intentionally invoked  

Questions begin to arise about **what is acceptable**, **what requires consent**, and **what should never be simulated**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used in immersive experiences to:

- dynamically adjust scenario difficulty or intensity  
- simulate emotionally realistic characters or agents  
- personalise challenges based on participant responses  
- adapt narratives in real time  

These uses matter because:

- AI may escalate intensity beyond what was anticipated  
- participants may feel manipulated rather than supported  
- emotional data may be inferred without consent  
- lines between learning and psychological exposure may blur  

This scenario treats ethical boundary-setting as **foundational**, not optional.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before deploying immersive experiences, teams should clarify:

- what kinds of responses the simulation is designed to evoke  
- what emotional, cognitive, or identity impacts are plausible  
- where the line lies between challenge and harm  

Key awareness questions:

- What are participants being asked to experience — not just do?  
- Could this scenario cause distress, shame, or coercion?  
- What assumptions are being made about resilience or consent?  

AI should never be used to push participants beyond ethical limits.

---

### 4.2 Human–AI Co-Agency

In immersive ethical contexts:

- humans retain responsibility for participant welfare  
- AI may support scenario variation within strict bounds  

Good co-agency means:

- ethical limits are human-defined and enforced  
- AI escalation rules are explicit and capped  
- facilitators can intervene or halt experiences immediately  

Avoid:

- allowing AI to determine emotional intensity autonomously  
- treating distress as a “learning outcome”  

---

### 4.3 Applied Practice

Appropriate AI uses include:

- adapting pacing while respecting consent boundaries  
- supporting optional branching paths  
- enabling opt-out or pause mechanisms  

Inappropriate uses include:

- surprise escalation without participant awareness  
- simulating trauma or humiliation  
- using AI to test emotional breaking points  

AI should support **agency and safety**, not endurance testing.

---

### 4.4 Ethics, Equity & Impact

Ethical risks are unevenly distributed.

Use the Framework to ask:

- Are some participants more vulnerable to harm?  
- Does this simulation assume cultural or emotional norms?  
- Could power dynamics make opting out feel unsafe?  

Ethical immersive design prioritises consent, dignity, and inclusion.

---

### 4.5 Decision-Making & Governance

Strong governance practices include:

- formal ethical review of immersive scenarios  
- documented boundaries and prohibited content  
- clear safeguarding and escalation protocols  

If AI is used:

- disclose adaptive features transparently  
- record design decisions and ethical trade-offs  
- ensure accountability for participant wellbeing  

This supports trust and institutional legitimacy.

---

### 4.6 Reflection, Learning & Renewal

After immersive experiences, reflect:

- Did participants feel safe and respected?  
- Where did ethical boundaries feel close or crossed?  
- What should be redesigned or restricted?  

Reflection ensures innovation remains humane and credible.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**

- Are participants consenting in practice, not just in theory?  
- Does challenge feel proportionate and purposeful?  
- Would we defend this experience publicly?  

**Optional AI prompts**

- “Monitor for signs of overload and flag for facilitator review.”  
- “Identify points where participants commonly disengage.”  

**Pause & check**

- Is this still learning — or has it become exposure?  
- Would stopping now be the responsible choice?

---

## 6. After-Action Reflection

Following immersive experiences:

- Were safeguards effective?  
- Did participants understand their rights and options?  
- How should ethical guidance evolve?  

Use insights to refine ethical standards and design practice.

---

## 7. What This Scenario Delivers

This scenario helps organisations:

- define ethical boundaries in immersive AI use  
- protect participant wellbeing and trust  
- avoid harm driven by novelty or technological power  
- strengthen governance of immersive innovation  
- embed ethics at the core of XR capability development  
