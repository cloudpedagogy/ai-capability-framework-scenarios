# Immersive Assessment and Performance Simulation: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports the use of **immersive simulations (VR/XR/AR) for assessment or performance evaluation**, where learner or professional actions within a simulated environment are observed, recorded, or interpreted as evidence of competence.

Immersive assessment can provide rich, contextual evidence that traditional assessments cannot. However, when AI is used to observe, score, interpret, or infer performance, the risks to fairness, validity, transparency, and trust increase significantly.

The purpose of this scenario is to help organisations use immersive and AI-supported assessment **responsibly and defensibly**, ensuring that assessment judgement remains human-led, criteria are explicit, and simulated performance is interpreted with care.

This scenario is designed to support:

- Assessment designers and programme leads  
- Professional accreditation and licensing bodies  
- Simulation and XR developers  
- External examiners and quality reviewers  
- Governance and academic integrity teams  

---

## 2. Situation & Context

An organisation is introducing immersive simulations as part of:

- formative or summative assessment  
- professional competence demonstration  
- skills-based evaluation (e.g. clinical, technical, leadership)  
- certification, accreditation, or readiness checks  

Within these environments:

- participant actions are tracked  
- decisions, timing, and responses are logged  
- AI may analyse patterns or flag performance indicators  

Decisions must now be made about **what counts as evidence**, **how performance is judged**, and **where AI fits into assessment authority**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used in immersive assessment to:

- track actions, sequences, or behaviours  
- flag errors, omissions, or delays  
- compare performance against benchmarks  
- generate performance summaries or scores  

These uses matter because:

- AI may privilege speed or efficiency over judgement  
- benchmarks may embed hidden norms or bias  
- inferred competence can exceed what evidence supports  
- automated scoring can obscure accountability  

This scenario treats AI-supported immersive assessment as **high-risk and high-stakes**, requiring robust governance.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using immersive assessment, teams should clarify:

- what capability is being assessed  
- what constitutes valid evidence in simulation  
- what cannot be inferred from simulated behaviour  

Key awareness questions:

- Are we assessing performance or proxy behaviour?  
- What assumptions are built into the simulation design?  
- Where might AI interpretation exceed evidence?  

AI should not be used to imply certainty where none exists.

---

### 4.2 Human–AI Co-Agency

In immersive assessment:

- humans retain authority over judgement and outcomes  
- AI may assist with observation and organisation  

Good co-agency means:

- assessment criteria are human-defined and explicit  
- AI outputs are advisory, not determinative  
- assessors understand how AI-derived indicators were produced  

Avoid:

- delegating pass/fail decisions to AI  
- allowing opaque scoring mechanisms  

---

### 4.3 Applied Practice

Appropriate AI uses include:

- highlighting areas for human review  
- supporting structured observation  
- assisting with formative feedback  

Inappropriate uses include:

- automated grading without moderation  
- using AI inference as sole evidence  
- extrapolating real-world competence from limited simulation  

AI should support **assessment literacy**, not replace judgement.

---

### 4.4 Ethics, Equity & Impact

Immersive assessment raises equity concerns.

Use the Framework to ask:

- Do all participants experience the simulation equally?  
- Are accessibility needs fully supported?  
- Could anxiety, motion sickness, or unfamiliarity affect outcomes?  

Ethical assessment requires accommodation, transparency, and proportionality.

---

### 4.5 Decision-Making & Governance

Strong governance practices include:

- clear documentation of assessment design  
- separation of formative feedback from summative judgement  
- auditability of AI-supported processes  

If AI is used:

- disclose its role in assessment clearly  
- document how indicators inform decisions  
- ensure appeal and review mechanisms exist  

This supports defensibility and institutional credibility.

---

### 4.6 Reflection, Learning & Renewal

After immersive assessment cycles, reflect:

- Did assessment outcomes align with intent?  
- Where did AI help or distort judgement?  
- What should be refined in future iterations?  

Reflection ensures assessment evolves responsibly.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**

- What evidence are we actually relying on here?  
- Are we over-weighting measurable behaviours?  
- Where is assessor judgement essential?  

**Optional AI prompts**

- “Highlight moments where assessor review is most needed.”  
- “Summarise observed behaviours without assigning scores.”  

**Pause & check**

- Would participants recognise this as fair?  
- Could this assessment be challenged — and defended?

---

## 6. After-Action Reflection

Following immersive assessment:

- Were results interpreted proportionately?  
- Did participants understand how they were assessed?  
- Where should governance be strengthened?  

Use insights to refine assessment design and oversight.

---

## 7. What This Scenario Delivers

This scenario helps organisations:

- design immersive assessments that are fair and defensible  
- avoid automation bias in performance evaluation  
- strengthen transparency and trust in AI-supported assessment  
- align immersive practice with governance expectations  
- build mature AI capability in high-stakes evaluation contexts  
