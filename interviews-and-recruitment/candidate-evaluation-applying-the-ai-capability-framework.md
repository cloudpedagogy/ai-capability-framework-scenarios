# Candidate Evaluation — Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports candidate evaluation during and immediately after interviews, where judgement is exercised under time pressure and where risks of bias, inconsistency, and over-reliance on summaries are highest.

It addresses a critical professional moment: synthesising evidence, comparing candidates fairly, and reaching defensible decisions — while resisting the temptation to use AI as a shortcut for judgement.

The purpose of this scenario is to help interview panels use AI, if at all, as a bounded sensemaking aid, while ensuring that accountability, fairness, and final decisions remain unequivocally human.

This scenario is designed to support:

- Interview panel members  
- Panel chairs  
- Hiring managers  
- HR and governance partners  

---

## 2. Situation and Context

Interviews have taken place, often across multiple days or panels. Panel members may be:

- holding handwritten or digital notes  
- recalling impressions unevenly  
- influenced by discussion dynamics or senior voices  

At this stage, common pressures include:

- fatigue and time constraints  
- desire for quick consensus  
- temptation to rely on summaries or rankings  

AI may be considered to help synthesise notes or compare candidates, but this is also the point at which misuse can most directly undermine fairness and accountability.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI is sometimes introduced at this stage to:

- summarise interview notes  
- compare candidate responses across criteria  
- highlight perceived strengths or gaps  

These uses matter because:

- summaries can introduce unintended weighting  
- comparative language can mask judgement calls  
- AI outputs may anchor panel discussion prematurely  

This scenario treats AI use in candidate evaluation as **high-risk**, requiring strict boundaries and explicit governance.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before any AI use, the panel should be clear about:

- what evidence was actually gathered  
- which criteria can be meaningfully compared  
- where uncertainty or disagreement remains  

Key awareness questions:

- Are we evaluating evidence or impressions?  
- What are we still unsure about?  
- Where might bias be influencing our recall?  

AI should not be used to resolve ambiguity — only to help surface it.

---

### 4.2 Human–AI Co-Agency

In candidate evaluation:

- humans must remain the sole decision-makers  
- AI, if used, may assist with organising information only  

Good co-agency means:

- AI never ranks or scores candidates  
- AI does not recommend a preferred candidate  
- panel discussion precedes any AI-supported synthesis  

The panel chair is responsible for maintaining these boundaries.

---

### 4.3 Applied Practice

Appropriate AI uses include:

- structuring notes by agreed criteria  
- identifying where evidence is missing or thin  
- highlighting inconsistencies in documentation  

Inappropriate uses include:

- generating comparative judgements  
- producing candidate rankings  
- predicting future performance  

AI outputs should be treated as neutral organisational aids, not evaluative judgements.

---

### 4.4 Ethics, Equity and Impact

This stage carries heightened ethical risk.

Key concerns include:

- confirmation bias  
- halo or horn effects  
- privileging confidence over substance  

Use the Framework to ask:

- Are all candidates being evaluated against the same criteria?  
- Are we over-weighting any single dimension?  
- Whose voice is dominating the discussion?  

AI cannot resolve these issues — but it can make them more visible.

---

### 4.5 Decision-Making and Governance

Strong governance practices include:

- documenting reasons for decisions  
- recording dissent or uncertainty where present  
- ensuring alignment with advertised criteria  

If AI is used:

- its role must be documented  
- outputs should not be retained as decision artefacts  
- transparency should be maintained in case of challenge  

This protects both candidates and the organisation.

---

### 4.6 Reflection, Learning and Renewal

After decisions are made, reflect:

- Where did judgement feel difficult?  
- Did AI help clarify or distract?  
- How could evaluation be improved next time?  

This reflection supports maturation of interview capability, not just individual outcomes.

---

## 5. In-the-Moment Prompts and Checks

### Human reflection prompts

- What evidence supports this judgement?  
- Are we applying criteria consistently?  
- What uncertainty are we glossing over?  

### Optional AI prompts

- “Organise these interview notes by the agreed evaluation criteria without summarising or ranking.”  
- “Highlight areas where evidence is missing or inconsistent across candidates.”  

### Pause and check

- Are we allowing any tool to decide on our behalf?  
- Would this process stand up to external scrutiny?

---

## 6. After-Action Reflection

Following appointment decisions:

- Did our evaluation process feel fair and defensible?  
- Where did disagreement arise, and why?  
- What should we change for future panels?  

Capture learning to improve future recruitment cycles.

---

## 7. What This Scenario Delivers

This scenario helps organisations to:

- support fair and accountable candidate evaluation  
- avoid over-reliance on AI at a critical decision point  
- strengthen panel judgement and discussion quality  
- reduce bias and inconsistency  
- build long-term AI capability through reflective practice
