# Post-Interview Reflection — Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports structured reflection after interview decisions have been made. It focuses on learning from the recruitment process itself — not revisiting individual candidates or outcomes — in order to strengthen fairness, consistency, and organisational AI capability over time.

Post-interview reflection is often skipped once an appointment is confirmed. When it does occur, it is usually informal and undocumented. This scenario reframes reflection as a deliberate capability practice, supported (carefully) by AI to surface patterns, blind spots, and opportunities for improvement.

The purpose is not audit or blame, but learning, maturity, and renewal.

This scenario is designed to support:

- Panel chairs  
- Hiring managers  
- HR and people partners  
- Academic and professional services leaders  

---

## 2. Situation and Context

Interviews are complete and an appointment decision has been made. Panel members may feel:

- relief that the process is finished  
- pressure to move quickly to onboarding  
- reluctance to revisit a demanding process  

At this stage:

- documentation already exists (notes, scoring, rationale)  
- emotions and impressions are still recent  
- insights about what worked — or didn’t — are accessible  

AI may be considered to help review process documentation or synthesise feedback, but reflection must remain process-focused, not candidate-focused.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used at this stage to:

- synthesise anonymised panel reflections  
- identify recurring issues or themes  
- compare intended vs actual use of criteria  
- surface process-level patterns across recruitments  

These uses matter because:

- reflection can otherwise rely on memory or anecdote  
- patterns of bias may go unnoticed  
- learning is easily lost between recruitment cycles  

This scenario treats AI use in post-interview reflection as **low-risk but high-value**, provided boundaries are clearly maintained.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Begin by clarifying:

- what aspects of the process are being reflected on  
- what evidence is available (notes, criteria, feedback)  
- what is explicitly out of scope (candidate re-evaluation)  

Key awareness questions:

- What were we trying to achieve with this recruitment?  
- Where did the process feel strong or weak?  
- What assumptions shaped our decisions?  

AI can help organise inputs, but cannot determine what matters.

---

### 4.2 Human–AI Co-Agency

In post-interview reflection:

- humans define learning goals  
- AI may assist with synthesis and pattern detection  

Good co-agency means:

- reflection questions are human-defined  
- AI outputs are reviewed collectively  
- accountability for interpretation remains human  

AI should never be used to retroactively justify decisions.

---

### 4.3 Applied Practice

Appropriate AI uses include:

- summarising anonymised process feedback  
- identifying recurring challenges across panels  
- comparing process design with stated intentions  

Inappropriate uses include:

- analysing individual candidate performance  
- revisiting or revising appointment decisions  
- ranking or scoring panels or panel members  

AI should support learning, not evaluation of people.

---

### 4.4 Ethics, Equity and Impact

Reflection is a key moment to address equity.

Use the Framework to ask:

- Did our process advantage or disadvantage particular groups?  
- Where might bias have entered despite our intentions?  
- Did AI use amplify or mitigate inequities?  

Ethical reflection focuses on systems and practices, not individuals.

---

### 4.5 Decision-Making and Governance

Good governance practices include:

- documenting key reflections and agreed improvements  
- ensuring reflections are stored appropriately  
- linking learning to future recruitment guidance  

If AI is used:

- document its role transparently  
- ensure outputs are anonymised  
- avoid retaining unnecessary data  

This supports institutional learning and accountability.

---

### 4.6 Reflection, Learning and Renewal

This scenario directly activates the Framework’s renewal domain.

Key renewal questions:

- What would we keep the same next time?  
- What would we change?  
- What capability has improved through this process?  

Over time, this builds organisational AI maturity, not just better hiring outcomes.

---

## 5. In-the-Moment Prompts and Checks

### Human reflection prompts

- Where did judgement feel hardest?  
- What surprised us about the process?  
- Where did AI help, and where did it complicate matters?  

### Optional AI prompts

- “Summarise anonymised panel reflections to identify recurring themes.”  
- “Highlight differences between intended and actual use of evaluation criteria.”  

### Pause and check

- Are we reflecting on the process, not the people?  
- Are we documenting learning in a way that improves future practice?

---

## 6. After-Action Reflection

Following reflection activities:

- What specific improvements will we implement next time?  
- Who is responsible for embedding these changes?  
- How will we know if capability has improved?  

Link reflection outputs to guidance, training, or templates as appropriate.

---

## 7. What This Scenario Delivers

This scenario helps organisations to:

- institutionalise learning from recruitment  
- strengthen fairness and consistency over time  
- use AI to support reflective practice responsibly  
- close the loop on AI capability development  
- move from one-off compliance to continuous improvement
