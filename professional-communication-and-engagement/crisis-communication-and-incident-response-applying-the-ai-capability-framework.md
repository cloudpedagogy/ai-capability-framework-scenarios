# Crisis Communication and Incident Response: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports crisis communication and incident response contexts, where organisations must respond quickly, clearly, and responsibly to events that carry reputational, legal, ethical, or human risk.

Examples include:
- data breaches or cybersecurity incidents
- safety failures or operational disruptions
- allegations of misconduct or harm
- research, teaching, or service incidents attracting public attention

AI is increasingly proposed in crisis contexts to draft holding statements, summarise facts, monitor media coverage, or prepare Q&A responses. While AI can support coordination under pressure, misuse can amplify harm, obscure accountability, or create legally and ethically problematic records.

The purpose of this scenario is to help organisations use AI, if at all, as a tightly bounded support tool during crises — while ensuring responsibility, judgement, and care remain decisively human.

This scenario is designed to support:
- Senior leaders and crisis response teams  
- Communications and media relations staff  
- Legal, risk, and governance officers  
- Academic and professional services leaders  

---

## 2. Situation & Context

A crisis or incident has occurred, or is unfolding. Characteristics often include:
- incomplete or rapidly changing information  
- heightened emotion, stress, or fear  
- pressure for immediate public response  
- internal disagreement about facts or responsibility  

At this stage:
- what is *not yet known* matters as much as what is known  
- early communications may be scrutinised later  
- records created during response may have legal significance  

AI may be introduced to accelerate drafting or synthesis, but speed must not override judgement, verification, or care.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used during crisis response to:
- draft holding statements or internal briefings  
- summarise incident timelines or known facts  
- prepare media Q&A or response options  
- monitor public or stakeholder reactions  

These uses matter because:
- AI may infer facts that are not verified  
- early drafts can harden into official positions  
- sentiment analysis can misrepresent harm or distress  

This scenario treats AI use in crisis contexts as **very high-risk**, requiring explicit boundaries and senior oversight.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI, teams must clarify:
- what is confirmed, unconfirmed, or unknown  
- what legal or ethical constraints apply  
- whose wellbeing and safety are at stake  

Key awareness questions:
- What do we *know for certain* right now?
- What must not be speculated on?
- Where could AI-generated text introduce false confidence?

AI must never be used to guess, infer, or “fill gaps” in crisis information.

---

### 4.2 Human–AI Co-Agency

In crisis response:
- humans retain full responsibility for judgement and decisions  
- AI may assist with organisation or drafting only  

Good co-agency means:
- AI outputs are clearly labelled as drafts  
- senior, accountable humans review all content  
- AI does not determine timing, tone, or disclosure  

Avoid:
- delegating crisis judgement to AI tools  
- using AI to depersonalise or distance responsibility  

---

### 4.3 Applied Practice

Appropriate AI uses include:
- structuring known facts with explicit uncertainty markers  
- drafting alternative holding statements for discussion  
- identifying unclear or risky wording in drafts  

Inappropriate uses include:
- generating definitive statements from partial data  
- speculating on causes or responsibility  
- automating responses to media or stakeholders  

AI should support caution and clarity — not urgency-driven overreach.

---

### 4.4 Ethics, Equity & Impact

Crises often involve harm, fear, or injustice.

Use the Framework to ask:
- Who is most affected by this incident?
- Does our language minimise harm or deflect responsibility?
- Are we prioritising reputation over people?

Ethical crisis communication centres care, transparency, and proportionality.

---

### 4.5 Decision-Making & Governance

Strong governance practices include:
- clear crisis decision authority and escalation routes  
- separation between drafting and approval  
- retention of records appropriate to legal risk  

If AI is used:
- document its role internally  
- ensure all outputs are human-approved  
- avoid creating unofficial or shadow records  

This supports auditability and organisational integrity.

---

### 4.6 Reflection, Learning & Renewal

After the immediate crisis phase, reflect:
- How did AI use affect clarity, speed, and care?
- Where did judgement feel most difficult?
- What safeguards should be strengthened?

Reflection supports institutional resilience, not just incident closure.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- What are we responsible for right now?
- What harm could this wording unintentionally cause?
- What would responsible restraint look like?

**Optional AI prompts**
- “Rewrite this draft to clearly separate confirmed facts from unknowns.”
- “Identify statements that could be interpreted as speculative.”

**Pause & check**
- Are we communicating care as well as information?
- Would we stand by this statement in hindsight?

---

## 6. After-Action Reflection

Following crisis response:
- Were communications accurate and proportionate?
- Where did misunderstandings arise?
- How should crisis AI guidance be updated?

Feed learning into crisis planning, training, and governance documentation.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- respond to crises with integrity and care  
- avoid AI-driven amplification of harm or risk  
- maintain accountability under pressure  
- strengthen crisis governance and preparedness  
- develop mature AI capability in high-stakes contexts
