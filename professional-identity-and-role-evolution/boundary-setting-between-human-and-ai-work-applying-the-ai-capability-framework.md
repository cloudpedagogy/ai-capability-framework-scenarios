# Boundary Setting Between Human and AI Work: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports professionals and teams who need to define, negotiate, and defend boundaries between **human responsibility** and **AI-supported work**.

As AI becomes embedded across professional roles, boundaries that were once implicit are becoming blurred. Tasks are increasingly shared, delegated, or augmented — often without explicit discussion of where responsibility, judgement, and accountability truly sit.

This scenario focuses on the **active practice of boundary setting**: deciding what should remain human-led, what may be AI-supported, and how those boundaries are communicated, reviewed, and upheld over time.

The purpose is to prevent silent drift toward over-delegation, while enabling confident, legitimate use of AI where it genuinely adds value.

This scenario is designed to support:
- Professionals working in AI-augmented roles  
- Team leads and managers setting expectations  
- Governance, QA, and risk owners  
- CPD and leadership development contexts  

---

## 2. Situation & Context

You work in a role where AI is now routinely available. Over time, boundaries have emerged informally:

- AI drafts first versions of work  
- AI summarises complex material  
- AI suggests options, rankings, or interpretations  
- Humans review, approve, or lightly edit outputs  

However:
- boundaries are rarely documented  
- expectations differ across teams  
- responsibility is sometimes ambiguous  
- pressure exists to “let AI handle more”  

You may notice:
- colleagues deferring judgement to AI suggestions  
- discomfort about where accountability truly lies  
- uncertainty about how much AI use is appropriate  
- difficulty explaining or justifying decisions externally  

You decide to use the AI Capability Framework to **make boundaries explicit, defensible, and shared**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used to:
- generate drafts, analyses, or recommendations  
- screen, rank, or prioritise options  
- identify patterns or risks  
- automate parts of professional workflows  

These uses matter because:
- boundary drift often happens incrementally  
- responsibility can become diffused  
- decision ownership may become unclear  
- governance and audit trails may weaken  

This scenario treats boundary setting as a **capability practice**, not a one-off policy decision.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Begin by identifying:
- where AI is currently used across your work  
- which uses were intentional vs. emergent  
- where boundaries feel unclear or contested  

Key awareness questions:
- What decisions are we implicitly delegating?
- Where does judgement still matter most?
- What assumptions are we making about AI reliability?

AI use without boundary awareness often feels efficient — until something goes wrong.

---

### 4.2 Human–AI Co-Agency

Boundary setting is fundamentally about agency.

In this scenario:
- humans must remain the accountable agents  
- AI acts as a bounded support tool  
- delegation is deliberate, not default  

Good co-agency means:
- humans define tasks before AI is involved  
- AI outputs never determine decisions alone  
- escalation points are clearly human-owned  

Avoid:
- treating AI recommendations as neutral  
- allowing convenience to override judgement  

---

### 4.3 Applied Practice

Effective boundary-setting practices include:
- explicitly naming which tasks are AI-supported  
- documenting decision points that remain human-only  
- pausing when AI outputs feel overly directive  
- agreeing team-level norms for AI use  

Inappropriate practices include:
- silent expansion of AI responsibility  
- inconsistent boundary enforcement  
- using AI to avoid difficult judgement calls  

Boundaries should be **visible in practice**, not just implied.

---

### 4.4 Ethics, Equity & Impact

Boundary decisions shape power and fairness.

Use the Framework to ask:
- Who benefits from looser boundaries?
- Who bears risk if AI use goes wrong?
- Are some roles disproportionately exposed?

Ethical boundary setting requires:
- attention to unequal authority and vulnerability  
- resistance to “automation by expectation”  
- support for colleagues raising concerns  

---

### 4.5 Decision-Making & Governance

Governance depends on clear boundaries.

Good practices include:
- documenting where AI informs decisions  
- naming accountable humans explicitly  
- aligning boundaries with institutional policy  

If AI is used:
- clarify authorship and responsibility  
- ensure decision trails remain intelligible  
- avoid creating plausible deniability  

Strong boundaries protect both individuals and institutions.

---

### 4.6 Reflection, Learning & Renewal

Boundaries must evolve.

Reflect on:
- Where did boundaries hold under pressure?
- Where did they erode?
- What new risks or opportunities emerged?

Renewal involves:
- revisiting boundaries periodically  
- updating shared understanding  
- strengthening confidence in judgement  

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- Who is accountable if this goes wrong?
- Are we comfortable owning this decision?
- What would responsible restraint look like?

**Optional AI prompts**
- “List assumptions underlying this recommendation.”
- “Identify uncertainties or limitations in this output.”

**Pause & check**
- Are we delegating judgement or just effort?
- Would this boundary withstand scrutiny?

---

## 6. After-Action Reflection

After AI-supported work:
- Did boundaries feel clear and respected?
- Was responsibility explicit?
- Did anyone feel pressured to over-delegate?

Capture:
- boundary decisions made  
- tensions encountered  
- lessons for future practice  

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- make human–AI boundaries explicit and defensible  
- prevent silent responsibility drift  
- strengthen governance and accountability  
- support confident, ethical AI use  
- build durable AI capability grounded in judgement  

