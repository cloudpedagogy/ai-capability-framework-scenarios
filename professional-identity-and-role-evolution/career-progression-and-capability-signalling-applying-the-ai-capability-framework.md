# Career Progression and Capability Signalling: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports professionals navigating **career progression, promotion, recognition, or role transition** in contexts where AI capability is increasingly visible, valued, or expected — but poorly defined.

As AI becomes embedded in professional work, individuals may feel pressure to:
- demonstrate AI fluency or innovation
- signal relevance or future-readiness
- differentiate themselves in competitive environments
- articulate AI capability credibly without exaggeration

This scenario focuses on **ethical, defensible capability signalling** — how professionals communicate what they can genuinely do with AI, what judgement they retain, and how their role is evolving.

It is designed to support:
- Professionals preparing for promotion or progression  
- Individuals articulating AI capability in CVs, portfolios, or reviews  
- Managers and panels evaluating AI-related capability claims  
- CPD and professional development contexts  

---

## 2. Situation & Context

You are preparing for a moment of evaluation or transition. This may include:
- applying for a new role or promotion
- preparing for appraisal or review
- repositioning after organisational change
- signalling readiness for future-oriented work

AI now appears explicitly or implicitly in expectations:
- job descriptions reference AI capability
- colleagues emphasise AI use or innovation
- leadership signals future skills priorities
- peers present confident AI narratives

At the same time:
- standards are inconsistent
- expectations are ambiguous
- exaggeration is common
- misuse or over-claiming carries reputational risk

This scenario addresses how to **signal AI capability with integrity**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used to:
- support work outputs cited as evidence
- assist with analysis, drafting, or planning
- shape workflows that underpin performance
- prepare applications or narratives

These uses matter because:
- outputs may reflect collaboration, not substitution
- claims may be scrutinised later
- AI involvement may be misunderstood
- signalling can drift into misrepresentation

Capability signalling is about **judgement, not tools**.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before signalling capability, clarify:
- what AI actually enables in your work
- where your judgement remains essential
- what you are accountable for

Key awareness questions:
- What can I do *with* AI that I could not do before?
- Where do I intervene, decide, or override?
- What risks or limits am I managing?

Strong signalling begins with self-understanding, not performance.

---

### 4.2 Human–AI Co-Agency

In capability narratives:
- humans remain the agents of value and judgement
- AI is positioned as a support, not a substitute
- responsibility is explicit

Good co-agency signalling means:
- describing how AI is used *within* practice
- naming where judgement is exercised
- avoiding claims of automation or replacement

Avoid:
- presenting AI outputs as personal expertise
- implying autonomy you do not control
- framing AI use as effortless mastery

---

### 4.3 Applied Practice

Credible capability signalling includes:
- concrete examples of AI-supported practice
- articulation of decision points
- reflection on learning and limits

Inappropriate signalling includes:
- listing tools without context
- inflating scope of responsibility
- claiming strategic impact without evidence

AI capability is demonstrated through **practice narratives**, not buzzwords.

---

### 4.4 Ethics, Equity & Impact

Capability signalling has equity implications.

Use the Framework to ask:
- Who benefits from confident signalling?
- Who may be disadvantaged by performative narratives?
- Does AI access vary across roles or contexts?

Ethical signalling:
- avoids creating unrealistic norms
- does not penalise cautious or reflective practice
- values learning and judgement, not bravado

---

### 4.5 Decision-Making & Governance

Organisations must evaluate AI capability responsibly.

Good governance practices include:
- clear criteria for AI-related capability
- avoidance of tool-centric assessment
- recognition of ethical and reflective practice

For individuals:
- ensure claims are defensible
- align narratives with role expectations
- retain evidence of practice and learning

Governance fails when signalling replaces substance.

---

### 4.6 Reflection, Learning & Renewal

Career signalling evolves over time.

Reflect on:
- how your role is genuinely changing
- what capability you are developing next
- where you need support or CPD

Renewal involves:
- updating narratives as practice matures
- letting go of performative pressure
- focusing on durable capability

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- Would I stand by this claim under scrutiny?
- What judgement am I demonstrating?
- Am I signalling confidence or capability?

**Optional AI prompts**
- “Help me articulate this example with clarity and limits.”
- “Identify where judgement and accountability appear in this narrative.”

**Pause & check**
- Is this honest, proportionate, and defensible?
- Does it reflect who I am becoming professionally?

---

## 6. After-Action Reflection

After progression or review moments:
- Were AI claims understood as intended?
- Did signalling feel authentic or performative?
- What would you change next time?

Capture learning to strengthen future articulation.

---

## 7. What This Scenario Delivers

This scenario helps organisations and individuals:
- articulate AI capability credibly and ethically
- reduce exaggeration and misrepresentation
- support fair evaluation of AI-augmented work
- align progression with judgement and accountability
- build reflective, future-ready professional identity

