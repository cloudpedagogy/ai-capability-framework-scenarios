# Post-Incident Review and Learning: Applying the AI Capability Framework

Framework: CloudPedagogy AI Capability Framework (2026 Edition)  
Licence: CC BY-NC-SA 4.0

---

## 1. Purpose of This Scenario

This scenario supports **post-incident review and learning** processes following operational, reputational, safety, data, or governance incidents where AI may have been used — or deliberately constrained — during response and decision-making.

Post-incident reviews are critical moments for organisational learning, accountability, and trust repair. However, they are often rushed, politicised, or reduced to surface-level compliance exercises. When AI has been involved, there is an added risk that:
- AI outputs are retrospectively treated as objective truth  
- responsibility becomes diffused or obscured  
- learning is replaced by justification or defensiveness  

The purpose of this scenario is to help organisations use AI, if at all, as a **support for structured reflection and learning**, while ensuring that responsibility, judgement, and meaning-making remain explicitly human.

This scenario is designed to support:
- Incident response leads and coordinators  
- Senior leaders and executive review panels  
- Risk, resilience, and assurance teams  
- Governance, audit, and compliance functions  
- Learning, quality, and organisational development teams  

---

## 2. Situation & Context

An incident has been resolved or stabilised. Immediate pressures have eased, and the organisation is moving into review mode.

At this stage:
- key decisions have already been made  
- AI may have been used for summarisation, coordination, drafting, or analysis  
- documentation, logs, and communications exist  
- emotional, reputational, or political sensitivities may still be present  

The review may be required for:
- internal learning and improvement  
- formal governance or audit processes  
- regulatory or external accountability  
- reputational recovery and trust rebuilding  

AI may be proposed to:
- summarise incident timelines  
- cluster lessons learned  
- analyse patterns across incidents  
- draft review reports or recommendations  

How AI is used in this phase will shape whether the review produces **genuine learning** or merely a defensible narrative.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used during post-incident review to:
- consolidate records, logs, and communications  
- generate timelines or summaries of events  
- identify recurring themes or systemic issues  
- draft learning reports or action plans  

These uses matter because:
- summaries can flatten disagreement or ambiguity  
- pattern detection can obscure context and causality  
- AI-generated narratives may appear authoritative  
- learning can be reframed as optimisation rather than judgement  

This scenario treats AI use in post-incident review as **medium-risk but high-consequence**, requiring careful framing and human oversight.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI in a post-incident review, teams should clarify:
- the purpose of the review (learning, accountability, assurance, improvement)  
- what questions are still open or contested  
- where interpretation, not synthesis, is required  

Key awareness questions:
- Are we trying to understand what happened, or to justify decisions?
- What uncertainties or disagreements remain?
- Where might AI outputs over-simplify causality?

AI should be used to **support recall and organisation**, not to determine meaning.

---

### 4.2 Human–AI Co-Agency

In post-incident review:
- humans remain responsible for interpretation and accountability  
- AI may assist with structuring material, not assigning meaning  

Good co-agency means:
- review questions are human-defined  
- AI outputs are treated as provisional artefacts  
- reviewers actively interrogate and contextualise outputs  

Avoid:
- treating AI summaries as definitive accounts  
- allowing AI to retrospectively rationalise decisions  

---

### 4.3 Applied Practice

Appropriate AI uses include:
- compiling timelines from verified records  
- clustering lessons for discussion  
- supporting drafting of review documents under supervision  

Inappropriate uses include:
- generating conclusions without deliberation  
- assigning blame or responsibility  
- ranking performance or decision quality  

AI should support **reflective learning**, not judgement replacement.

---

### 4.4 Ethics, Equity & Impact

Post-incident reviews can have significant human impact.
Use the Framework to ask:
- Whose perspectives are represented or missing?
- Could AI synthesis silence minority or dissenting views?
- Are we learning in ways that reduce future harm?

Ethical review prioritises care, fairness, and psychological safety.

---

### 4.5 Decision-Making & Governance

Strong governance in post-incident review includes:
- clear ownership of findings and recommendations  
- separation between factual record and interpretive judgement  
- transparent documentation of learning and follow-up actions  

If AI is used:
- record its role in review processes  
- ensure human validation of findings  
- avoid opaque or non-auditable review artefacts  

This supports legitimacy, defensibility, and trust.

---

### 4.6 Reflection, Learning & Renewal

This scenario explicitly activates the **Reflection, Learning & Renewal** domain.

Key renewal questions:
- What capabilities were strengthened or exposed?
- What assumptions need revisiting?
- How should AI use be bounded differently next time?

Learning should inform:
- incident response protocols  
- training and simulations  
- governance thresholds  
- future AI use boundaries  

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- What surprised us most about this incident?
- Where did judgement matter more than information?
- What would we do differently under similar pressure?

**Optional AI prompts**
- “Summarise incident records, highlighting areas of uncertainty or disagreement.”
- “Identify recurring themes across incidents without assigning causality.”

**Pause & check**
- Are we learning, or defending?
- Is AI helping surface insight, or smoothing discomfort?

---

## 6. After-Action Reflection

Following the review:
- Are learning outcomes clearly articulated?
- Are responsibilities for follow-up explicit?
- How will learning be shared and revisited?

Ensure learning feeds into:
- training and tabletop exercises  
- governance updates  
- scenario planning and resilience work  

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- conduct credible, learning-focused post-incident reviews  
- avoid retrospective automation bias  
- maintain human accountability and ethical reflection  
- strengthen organisational resilience  
- embed AI capability as a learning practice, not a liability  

---

