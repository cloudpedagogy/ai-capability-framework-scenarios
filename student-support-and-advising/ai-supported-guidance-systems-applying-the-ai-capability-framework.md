# AI-Supported Guidance Systems: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports the design, deployment, and use of AI-supported guidance systems that provide students with information, recommendations, or direction related to study choices, progression, support services, or academic planning.

These systems may take the form of chatbots, recommender systems, interactive FAQs, or integrated advising tools. They are often positioned as scalable solutions to growing student demand. However, they operate in high-trust contexts where advice can shape decisions with long-term academic, financial, and personal consequences.

The purpose of this scenario is to help institutions use AI-supported guidance systems responsibly — ensuring that advice remains bounded, transparent, and accountable, and that students retain access to human judgement and care.

This scenario is designed to support:
- Student services and advising teams  
- Digital education and IT leads  
- Academic governance and quality teams  
- Designers of AI-enabled student systems  

---

## 2. Situation & Context

An institution introduces or expands an AI-supported guidance system to help students:

- navigate programmes and pathways  
- understand regulations or requirements  
- identify support services  
- explore options for progression or change  

The system may:
- draw on institutional data and policies  
- use natural language interfaces  
- generate personalised responses  

Students may perceive the system as:
- authoritative  
- official  
- a substitute for human advice  

How the system is framed and governed will determine whether it empowers students or misleads them.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI-supported guidance systems may:
- answer student questions conversationally  
- recommend actions or options  
- summarise policies or requirements  
- triage students to services  

These uses matter because:
- generated advice can appear definitive  
- errors or oversimplifications may go unnoticed  
- students may not distinguish guidance from decision  
- responsibility for advice can become diffuse  

This scenario treats AI-supported guidance as **high-impact and ethically sensitive**, even where formal risk appears low.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before deploying or using AI-supported guidance, clarify:

- what the system is allowed to advise on  
- what it must explicitly not advise on  
- how uncertainty and exceptions are handled  

Key awareness questions:
- What decisions might students make based on this advice?
- Where could misinterpretation cause harm?
- What assumptions are embedded in the system’s responses?

AI should inform navigation, not replace advising judgement.

---

### 4.2 Human–AI Co-Agency

In guidance contexts:

- humans remain responsible for advice quality and boundaries  
- AI acts as a signposting and information aid  

Good co-agency means:
- guidance systems clearly defer to human advisers  
- escalation pathways are explicit and accessible  
- staff understand how the system operates  

Avoid:
- presenting AI guidance as authoritative  
- allowing students to believe advice is final  

---

### 4.3 Applied Practice

Appropriate AI uses include:
- answering factual, low-stakes questions  
- directing students to policies or services  
- clarifying terminology or processes  

Inappropriate uses include:
- advising on personal, financial, or mental health decisions  
- recommending irreversible academic actions  
- interpreting individual circumstances without context  

AI guidance should be conservative, bounded, and reversible.

---

### 4.4 Ethics, Equity & Impact

Guidance systems affect access and opportunity.

Use the Framework to ask:
- Are all students equally served by this system?
- Does language, tone, or design privilege some users?
- Are students aware of the system’s limitations?

Ethical guidance design prioritises inclusion, clarity, and safety.

---

### 4.5 Decision-Making & Governance

Strong governance includes:
- approval and oversight of guidance scope  
- clear accountability for content and updates  
- alignment with student support policies  

If AI is used:
- document decision boundaries  
- review outputs regularly  
- provide audit trails for system behaviour  

This supports defensibility and institutional trust.

---

### 4.6 Reflection, Learning & Renewal

After deployment, reflect:
- How are students actually using the system?
- Where are misunderstandings occurring?
- What guidance should be withdrawn or refined?

Reflection supports responsible evolution rather than silent drift.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- What might a student infer from this advice?
- Where should we slow down or redirect?
- Is human support clearly visible?

**Optional AI prompts**
- “Answer this question with explicit uncertainty and referral to human support.”
- “List limitations of this guidance clearly.”

**Pause & check**
- Would we be comfortable defending this advice publicly?
- Is responsibility clearly human-owned?

---

## 6. After-Action Reflection

Following implementation or review:
- Did the system reduce or increase advising burden?
- Were escalation routes used appropriately?
- Did trust increase or erode?

Use insights to recalibrate system scope and messaging.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- deploy AI guidance without overreach  
- protect students from misdirected advice  
- maintain accountability and trust  
- align automation with care-oriented practice  
- build mature AI capability in student support systems
