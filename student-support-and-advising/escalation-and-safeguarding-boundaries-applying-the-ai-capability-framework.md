# Escalation and Safeguarding Boundaries: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports situations where concerns about student wellbeing, safety, or risk escalate beyond routine support and require **formal safeguarding processes**. It focuses on defining and maintaining clear boundaries between AI-enabled systems and human-led safeguarding judgement.

Escalation is the point at which responsibility becomes explicit, documented, and accountable. When AI is involved — through monitoring systems, alerts, or triage mechanisms — there is a significant risk of confusion about who is responsible for action, delay, or oversight.

The purpose of this scenario is to help institutions ensure that AI use **supports early awareness and routing**, while safeguarding decisions, thresholds, and actions remain **unequivocally human-led**.

This scenario is designed to support:
- Safeguarding and student welfare leads  
- Academic staff with escalation responsibilities  
- Student services and wellbeing teams  
- Designers, approvers, and governors of AI-enabled systems  

---

## 2. Situation & Context

An institution operates formal safeguarding processes for:
- mental health crises  
- risk of harm to self or others  
- serious welfare concerns  
- legal or duty-of-care obligations  

AI may already be present in the environment, including:
- engagement or attendance monitoring  
- sentiment analysis or flagging systems  
- automated alerts or dashboards  
- front-line chat or guidance tools  

A concern arises that may require escalation. At this point:
- time sensitivity increases  
- accountability must be clear  
- ambiguity becomes dangerous  

How AI is used — or excluded — at this stage is critical.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used to:
- flag potential safeguarding concerns  
- route information to appropriate teams  
- support record-keeping or timelines  

These uses matter because:
- escalation thresholds are normative and contextual  
- misrouting or delay can cause harm  
- automated escalation can obscure responsibility  

This scenario treats AI use in safeguarding escalation as **very high-risk**, requiring strict boundaries and conservative design.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before involving AI in escalation pathways, clarify:

- what constitutes a safeguarding concern  
- where AI detection ends and human judgement begins  
- which risks are unacceptable  

Key awareness questions:
- Could AI silence or delay human concern?
- Are we mistaking detection for responsibility?
- What must never be automated?

AI should never determine whether safeguarding action is required.

---

### 4.2 Human–AI Co-Agency

In safeguarding contexts:

- humans hold sole authority for escalation decisions  
- AI may assist with notification or visibility  

Good co-agency means:
- escalation authority is clearly assigned to named roles  
- AI outputs are advisory, not directive  
- responsibility cannot be deflected to systems  

Avoid:
- automated escalation without human validation  
- reliance on AI confidence or thresholds  

---

### 4.3 Applied Practice

Appropriate AI uses include:
- alerting staff to patterns requiring attention  
- consolidating information for review  
- supporting timely communication  

Inappropriate uses include:
- triggering safeguarding actions autonomously  
- classifying severity or urgency  
- suppressing alerts deemed “low confidence”  

AI should support vigilance, not replace judgement.

---

### 4.4 Ethics, Equity & Impact

Safeguarding decisions affect lives.

Use the Framework to ask:
- Are some students more likely to be escalated unfairly?
- Could AI use amplify bias or profiling?
- Are students aware of monitoring boundaries?

Ethical escalation prioritises proportionality, fairness, and transparency.

---

### 4.5 Decision-Making & Governance

Strong governance requires:
- explicit prohibition of AI-led safeguarding decisions  
- clear escalation maps and ownership  
- audit trails that show human judgement  

If AI is used:
- document its limited role clearly  
- train staff on interpretation and limits  
- review incidents involving AI involvement  

This ensures legal defensibility and moral responsibility.

---

### 4.6 Reflection, Learning & Renewal

After safeguarding reviews, reflect:
- Did AI involvement help or hinder response?
- Were boundaries respected under pressure?
- Should AI use be reduced or redesigned?

Learning here must prioritise safety over innovation.

---

## 5. In-the-Moment Prompts & Checks

**Human reflection prompts**
- Who is accountable right now?
- Is this decision defensible without reference to AI?
- Are we acting soon enough?

**Optional AI prompts**
- “Summarise relevant signals for human review without ranking or judgement.”
- “Highlight missing information needed for escalation.”

**Pause & check**
- Are we expecting AI to carry responsibility it cannot bear?
- Would we accept full accountability for this outcome?

---

## 6. After-Action Reflection

Following safeguarding cases:
- Were escalation pathways clear and effective?
- Did AI involvement introduce ambiguity?
- What boundaries need reinforcing?

Feed learning into policy, training, and system design.

---

## 7. What This Scenario Delivers

This scenario helps organisations:
- maintain clear safeguarding accountability  
- prevent AI overreach in high-risk contexts  
- protect students and staff legally and ethically  
- align AI use with duty-of-care obligations  
- build trust in support and escalation systems
