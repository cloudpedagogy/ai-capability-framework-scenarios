# Supporting Students in Distress: Applying the AI Capability Framework

**Framework:** CloudPedagogy AI Capability Framework (2026 Edition)  
**Licence:** CC BY-NC-SA 4.0  

---

## 1. Purpose of This Scenario

This scenario supports contexts where students may be experiencing distress, vulnerability, or crisis, and where AI systems are present in the ecosystem of support — whether through chatbots, monitoring tools, guidance systems, or staff-facing dashboards.

Supporting students in distress is a fundamentally human, care-oriented responsibility. When AI is introduced into this space, the risks are significant: misinterpretation, delayed intervention, inappropriate reassurance, or the erosion of trust can cause real harm.

The purpose of this scenario is to help institutions use AI, if at all, in ways that **support recognition, signposting, and human response**, while ensuring that care, judgement, and responsibility remain explicitly human.

This scenario is designed to support:
- Student wellbeing and support services  
- Academic staff with pastoral responsibilities  
- Safeguarding and risk management teams  
- Designers and governors of AI-enabled student systems  

---

## 2. Situation & Context

An institution operates in an environment where:
- students may experience mental health challenges, distress, or crisis  
- staff capacity for pastoral support is stretched  
- digital systems increasingly mediate student interaction  

AI may appear in the form of:
- wellbeing chatbots or check-ins  
- sentiment analysis or engagement monitoring  
- automated signposting to support services  

Students may:
- disclose distress indirectly or ambiguously  
- test boundaries before seeking help  
- assume systems are monitored or responsive  

How AI is framed and governed here can affect **safety, trust, and willingness to seek help**.

---

## 3. Where AI Might Be Used (and Why That Matters)

AI may be used to:
- detect indicators of disengagement or distress  
- respond to student queries about support  
- route students to services  
- flag concerns to staff  

These uses matter because:
- distress is contextual and nuanced  
- false negatives and false positives both carry risk  
- students may interpret AI responses as care  

This scenario treats AI use in distress contexts as **high-risk, high-responsibility**, requiring conservative design and strong safeguards.

---

## 4. Applying the AI Capability Framework

### 4.1 Awareness

Before using AI in distress-related contexts, clarify:

- what signals the system is allowed to act on  
- what it cannot reliably detect  
- how uncertainty and ambiguity are handled  

Key awareness questions:
- What could go wrong if this system misinterprets distress?
- What assumptions are we making about student behaviour?
- Where must human judgement always intervene?

AI should support noticing and referral — never diagnosis or reassurance.

---

### 4.2 Human–AI Co-Agency

In wellbeing and distress contexts:

- humans retain full responsibility for care and safeguarding  
- AI may assist with signposting or alerting  

Good co-agency means:
- AI never presents itself as a support substitute  
- escalation to humans is immediate and visible  
- staff understand system limits and triggers  

Avoid:
- conversational AI offering emotional reassurance  
- systems that appear to “handle” distress independently  

---

### 4.3 Applied Practice

Appropriate AI uses include:
- highlighting potential concerns for human review  
- providing clear, factual information about support options  
- encouraging contact with human services  

Inappropriate uses include:
- assessing severity of distress  
- delaying human contact through automation  
- offering advice or coping strategies autonomously  

AI should shorten the path to care, not interpose itself within it.

---

### 4.4 Ethics, Equity & Impact

Distress contexts amplify ethical responsibility.

Use the Framework to ask:
- Are some students more likely to be misread or missed?
- Could surveillance perceptions discourage help-seeking?
- How is consent and transparency handled?

Ethical practice prioritises dignity, privacy, and trust.

---

### 4.5 Decision-Making & Governance

Strong governance includes:
- explicit prohibition of AI diagnosis or counselling  
- alignment with safeguarding and wellbeing policies  
- defined accountability for intervention decisions  

If AI is used:
- document triggers and escalation logic  
- ensure staff training and oversight  
- regularly audit outcomes and incidents  

This supports defensibility and protects students and staff.

---

### 4.6 Reflection, Learning & Renewal

After incidents or review cycles, reflect:
- Did AI use support timely human intervention?
- Where did it introduce confusion or delay?
- What should be withdrawn, tightened, or redesigned?

Reflection here is a matter of safety, not optimisation.

---

## 5. In-the-
